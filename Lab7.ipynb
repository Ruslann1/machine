{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition: Lab 7\n",
    "### Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x*(1.0-x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqigYRESkioJBRESq/H+bas/xrjIM3AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9afa02e4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [ 0.02332909  0.01048178  0.01638983]\n",
      "[0 1] [ 0.57158165  0.57214226  0.57103026]\n",
      "[1 0] [ 0.98274001  0.99234993  0.98781302]\n",
      "[1 1] [ 0.57212389  0.57133833  0.57227481]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "\n",
    "# Set weights\n",
    "        self.weights = []\n",
    "# layers = [2,2,1]\n",
    "# range of weight values (-1,1)\n",
    "# input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "# output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "# Add column of ones to X\n",
    "# This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        myList=[]\n",
    "        avList=[]\n",
    "\n",
    "\n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "    \n",
    "            for l in range(len(self.weights)):\n",
    "                dot_value = np.dot(a[l], self.weights[l])\n",
    "                activation = self.activation(dot_value)\n",
    "                a.append(activation)\n",
    "# output layer\n",
    "\n",
    "\n",
    "            error = y[i] - a[-1]\n",
    "            myList.append(np.sum(error**2))# mean squard error MSE\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# we need to begin at the second to last layer\n",
    "# (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "# reverse\n",
    "# [level3(output)->level2(hidden)] => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "# backpropagation\n",
    "# 1. Multiply its output delta and input activation\n",
    "# to get the gradient of the weight.\n",
    "# 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "\n",
    "            cnt = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                cnt =cnt + layer.T.dot(delta)\n",
    "                self.weights[i] =self.weights[i]+ learning_rate * cnt# save in smthing momentum\n",
    "            t = np.average(myList)\n",
    "            avList.append(t)\n",
    "            if k % 10000 == 0:\n",
    "                print('epochs:', k)\n",
    "                t = np.average(myList)\n",
    "                avList.append(t)\n",
    "\n",
    "# plt.show()\n",
    "#print(myList)\n",
    "\n",
    "#plt.plot(myList[1])\n",
    "        plt.plot(avList)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('error')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))\n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "# X = np.array([[-1, -1],\n",
    "# [-1, 1],\n",
    "# [1, -1],\n",
    "# [1, 1]])\n",
    "# y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
